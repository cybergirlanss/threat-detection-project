{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import combinations\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from pytorchvideo.models.hub import c2d_r50\n",
    "\n",
    "# -------------------- Annotation Processing -------------------- #\n",
    "class AnnotationParser:\n",
    "    def __init__(self, annotation_path):\n",
    "        self.annotations = self._parse_annotations(annotation_path)\n",
    "        \n",
    "    def _parse_annotations(self, path):\n",
    "        \"\"\"Parse annotations in Temporal_Anomaly_Annotation.txt format\"\"\"\n",
    "        annotations = {}\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 6:\n",
    "                    continue\n",
    "                \n",
    "                video_name = parts[0]\n",
    "                label = self._extract_label(video_name)\n",
    "                segments = []\n",
    "                \n",
    "                # Process all frame pairs\n",
    "                for i in range(2, len(parts), 2):\n",
    "                    start = int(parts[i])\n",
    "                    end = int(parts[i+1])\n",
    "                    if start == -1 or end == -1:\n",
    "                        continue\n",
    "                    segments.append({'start_frame': start, 'end_frame': end})\n",
    "                \n",
    "                annotations[video_name] = {\n",
    "                    'label': label,\n",
    "                    'segments': segments,\n",
    "                    'event_type': parts[1]\n",
    "                }\n",
    "        return annotations\n",
    "    \n",
    "    def _extract_label(self, name):\n",
    "        \"\"\"Extract label from video name up to first digit\"\"\"\n",
    "        match = re.match(r'^[^\\d]*', name)\n",
    "        return match.group(0).rstrip('_') if match else 'unknown'\n",
    "\n",
    "# -------------------- Interaction Tracking -------------------- #\n",
    "class InteractionTracker:\n",
    "    def __init__(self, window_size=30):\n",
    "        self.history = deque(maxlen=window_size)\n",
    "        \n",
    "    def update(self, current_features):\n",
    "        \"\"\"Track temporal interaction patterns\"\"\"\n",
    "        \"\"\"Track temporal interaction patterns with numpy conversion\"\"\"\n",
    "        # Convert features to numpy array\n",
    "        current_arr = np.array(current_features, dtype=np.float32)\n",
    "        features = {\n",
    "            'current': current_arr,\n",
    "            'velocity': np.zeros_like(current_arr),\n",
    "            'acceleration': np.zeros_like(current_arr),\n",
    "            'duration': 0\n",
    "        }\n",
    "        \n",
    "        if len(self.history) >= 1:\n",
    "            features['velocity'] = current_arr - self.history[-1]\n",
    "            \n",
    "            if len(self.history) >= 2:\n",
    "                prev_velocity = self.history[-1] - self.history[-2]\n",
    "                features['acceleration'] = features['velocity'] - prev_velocity\n",
    "                \n",
    "        features['duration'] = len(self.history)\n",
    "        self.history.append(current_arr)\n",
    "        return features\n",
    "    \n",
    "    def group_analysis(self, people):\n",
    "        \"\"\"Analyze group dynamics between 3+ people\"\"\"\n",
    "        if len(people) < 3:\n",
    "            return {\n",
    "                'social_force': -1.0,\n",
    "                'dominance': -1,\n",
    "                'formation': -1.0\n",
    "            }\n",
    "            \n",
    "        positions = np.array([p['bbox_center'] for p in people])\n",
    "        centroid = np.mean(positions, axis=0)\n",
    "        \n",
    "        return {\n",
    "            'social_force': self._calc_social_force(positions),\n",
    "            'dominance': self._calc_dominance(positions, centroid),\n",
    "            'formation': self._detect_formation(positions)\n",
    "        }\n",
    "    \n",
    "    def _calc_social_force(self, positions):\n",
    "        forces = []\n",
    "        for i in range(len(positions)):\n",
    "            others = np.delete(positions, i, axis=0)\n",
    "            diffs = others - positions[i]\n",
    "            dists = np.linalg.norm(diffs, axis=1)\n",
    "            forces.append(np.sum(1 / (dists + 1e-9)))\n",
    "        return np.mean(forces)\n",
    "    \n",
    "    def _calc_dominance(self, positions, centroid):\n",
    "        return np.linalg.norm(positions - centroid, axis=1).argmin()\n",
    "    \n",
    "    def _detect_formation(self, positions):\n",
    "        angles = []\n",
    "        for trio in combinations(positions, 3):\n",
    "            v1 = trio[1] - trio[0]\n",
    "            v2 = trio[2] - trio[0]\n",
    "            angles.append(np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2) + 1e-9)))\n",
    "        return np.mean(angles)\n",
    "\n",
    "# -------------------- Main Processing Pipeline -------------------- #\n",
    "class EnhancedInteractionPipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.yolo = YOLO('yolov8x-seg.pt')\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.c3d_model = c2d_r50(pretrained=True).eval()\n",
    "        self.tracker = InteractionTracker()\n",
    "        self._setup_directories()\n",
    "        self.frame_buffer = deque(maxlen=16)  # Temporal window for C3D\n",
    "        \n",
    "    def _setup_directories(self):\n",
    "        os.makedirs(self.config['frame_dir'], exist_ok=True)\n",
    "        os.makedirs(self.config['feature_dir'], exist_ok=True)\n",
    "        os.makedirs(self.config['metadata_dir'], exist_ok=True)\n",
    "        \n",
    "    def process_videos(self, annotation_path):\n",
    "        parser = AnnotationParser(annotation_path)\n",
    "        for video_name, data in parser.annotations.items():\n",
    "            video_path = os.path.join(self.config['video_dir'], video_name)\n",
    "            if not os.path.exists(video_path):\n",
    "                continue\n",
    "                \n",
    "            self._process_video(video_path, data)\n",
    "    \n",
    "    def _process_video(self, video_path, video_data):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cap.release()\n",
    "        self.frame_buffer.clear()  # Reset buffer for new video\n",
    "        \n",
    "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "        for seg_idx, segment in enumerate(video_data['segments']):\n",
    "            # Handle normal videos\n",
    "            if segment['start_frame'] == -1:\n",
    "                start_frame = 0\n",
    "                end_frame = total_frames - 1\n",
    "            else:\n",
    "                start_frame = segment['start_frame']\n",
    "                end_frame = segment['end_frame']\n",
    "            \n",
    "            frames = self._extract_frames(video_path, start_frame, end_frame)\n",
    "            features = []\n",
    "            \n",
    "            for frame_idx, frame in enumerate(tqdm(frames)):\n",
    "                \n",
    "                # Detect people\n",
    "                detections = self._detect_people(frame)\n",
    "                \n",
    "                # Process keypoints\n",
    "                keypoints = []\n",
    "                for person in detections:\n",
    "                    person['keypoints'] = self._get_pose_keypoints(\n",
    "                        person['cropped_frame'])\n",
    "                    keypoints.append(person['keypoints'])\n",
    "                \n",
    "                # Calculate interactions\n",
    "                interactions = self._calculate_interactions(detections, keypoints)\n",
    "                \n",
    "                # Extract temporal features\n",
    "                temporal_features = self._get_temporal_features(frame)\n",
    "                \n",
    "                # Track interactions\n",
    "                tracked_features = self.tracker.update(interactions)\n",
    "                group_features = self.tracker.group_analysis(detections)\n",
    "                \n",
    "                # Save data\n",
    "                self._save_frame(frame, video_name, seg_idx, frame_idx)\n",
    "                features.append({\n",
    "                    'spatial': interactions,\n",
    "                    'temporal': temporal_features,\n",
    "                    'tracked': tracked_features,\n",
    "                    'group': group_features\n",
    "                })\n",
    "            \n",
    "            # Save features and metadata\n",
    "            self._save_features(features, video_name, seg_idx, video_data['label'])\n",
    "            self._save_metadata(video_name, seg_idx, {\n",
    "                'resolution': frame.shape[:2],\n",
    "                'fps': fps,\n",
    "                'num_people': [len(f['spatial']) for f in features],\n",
    "                'event_type': video_data['event_type']\n",
    "            })\n",
    "    \n",
    "    def _extract_frames(self, video_path, start_frame, end_frame):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        \n",
    "        while cap.isOpened() and len(frames) <= (end_frame - start_frame):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def _detect_people(self, frame):\n",
    "        results = self.yolo(frame)[0]\n",
    "        people = []\n",
    "        \n",
    "        if results.boxes is None or len(results.boxes) == 0:\n",
    "            return people\n",
    "        \n",
    "        person_indices = [\n",
    "            i for i, cls in enumerate(results.boxes.cls)\n",
    "            if int(cls) == 0  # Person class\n",
    "        ]\n",
    "        \n",
    "        for idx in person_indices:\n",
    "            box = results.boxes[idx]\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            \n",
    "            mask = None\n",
    "            if results.masks and idx < len(results.masks):\n",
    "                mask = results.masks[idx].data[0].cpu().numpy()\n",
    "            \n",
    "            cropped = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            people.append({\n",
    "                'bbox': [x1, y1, x2, y2],\n",
    "                'bbox_center': [(x1+x2)//2, (y1+y2)//2],\n",
    "                'cropped_frame': cropped,\n",
    "                'mask': mask,\n",
    "                'confidence': float(box.conf)\n",
    "            })\n",
    "        \n",
    "        return people\n",
    "    \n",
    "    def _get_pose_keypoints(self, cropped_frame):\n",
    "        \"\"\"Get pose landmarks using MediaPipe\"\"\"\n",
    "        with self.mp_pose.Pose(\n",
    "            static_image_mode=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            model_complexity=2) as pose:\n",
    "            \n",
    "            results = pose.process(cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB))\n",
    "            if not results.pose_landmarks:\n",
    "                return np.zeros((33, 4))\n",
    "            \n",
    "            return np.array([[lm.x, lm.y, lm.z, lm.visibility]\n",
    "                for lm in results.pose_landmarks.landmark])\n",
    "    \n",
    "    def _calculate_interactions(self, detections, keypoints):\n",
    "        interactions = []\n",
    "        for i, j in combinations(range(len(detections)), 2):\n",
    "            kp1 = keypoints[i]\n",
    "            kp2 = keypoints[j]\n",
    "            \n",
    "            interaction = {\n",
    "                'pair': (i, j),\n",
    "                'distance': {\n",
    "                    'wrist_nose': np.linalg.norm(kp1[15][:3] - kp2[0][:3]),\n",
    "                    'ankles': np.linalg.norm(kp1[27][:3] - kp2[27][:3])\n",
    "                },\n",
    "                'angle': {\n",
    "                    'shoulder_hip': self._vector_angle(\n",
    "                        kp1[11][:3] - kp1[23][:3],\n",
    "                        kp2[12][:3] - kp2[24][:3])\n",
    "                },\n",
    "                'visibility': (kp1[:, 3].mean() + kp2[:, 3].mean()) / 2\n",
    "            }\n",
    "            interactions.append(interaction)\n",
    "        return interactions\n",
    "    \n",
    "    def _vector_angle(self, v1, v2):\n",
    "        return np.arccos(\n",
    "            np.dot(v1, v2) / \n",
    "            (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-9)\n",
    "        )\n",
    "    \n",
    "    def _get_temporal_features(self, frame):\n",
    "        \"\"\"Process frame sequences for C3D with proper dimensions\"\"\"\n",
    "        # Convert frame to tensor and add to buffer\n",
    "        frame_tensor = torch.from_numpy(frame).permute(2, 0, 1)  # [C, H, W]\n",
    "        self.frame_buffer.append(frame_tensor)\n",
    "        \n",
    "        # Wait until we have enough frames\n",
    "        if len(self.frame_buffer) < 16:\n",
    "            return np.zeros(2048)  # Return zeros until buffer fills\n",
    "            \n",
    "        # Create proper 5D tensor [1, C, T, H, W]\n",
    "        sequence = torch.stack(list(self.frame_buffer), dim=1)  # [C, T, H, W]\n",
    "        sequence = sequence.unsqueeze(0)  # [1, C, T, H, W]\n",
    "        \n",
    "        # Handle channel mismatch\n",
    "        if sequence.shape[1] != 3:\n",
    "            sequence = sequence[:, :3]  # Take first 3 channels if needed\n",
    "            \n",
    "        # Verify final dimensions\n",
    "        if sequence.shape[1:] != (3, 16, 112, 112):\n",
    "            sequence = F.interpolate(sequence, size=(112, 112))\n",
    "            \n",
    "        return self.c3d_model(sequence).squeeze().detach().numpy()\n",
    "        # print(\"Frame shape before processing:\", frame.shape)  # Debug\n",
    "        # # Convert to 5D tensor (batch, channels, time, height, width)\n",
    "        # if len(frame.shape) == 2 or frame.shape[-1] == 1:  \n",
    "        #     print(\"Warning: Frame is grayscale. Converting to RGB...\")\n",
    "        #     frame = np.stack([frame] * 3, axis=-1)  # Convert single-channel grayscale to 3-channel RGB\n",
    "        # frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).unsqueeze(0).unsqueeze(0)\n",
    "        # print(\"Frame tensor shape after processing:\", frame_tensor.shape)  # Debug\n",
    "        # # assert frame_tensor.shape[1] == 3, \"Frame should have 3 channels!\"\n",
    "        # return self.c3d_model(frame_tensor).detach().numpy()\n",
    "\n",
    "    \n",
    "    def _save_frame(self, frame, video_name, segment_idx, frame_idx):\n",
    "        frame_dir = os.path.join(\n",
    "            self.config['frame_dir'],\n",
    "            f\"{video_name}_seg{segment_idx}\"\n",
    "        )\n",
    "        os.makedirs(frame_dir, exist_ok=True)\n",
    "        cv2.imwrite(\n",
    "            os.path.join(frame_dir, f\"frame_{frame_idx:04d}.jpg\"),\n",
    "            cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        )\n",
    "    \n",
    "    def _save_features(self, features, video_name, segment_idx, label):\n",
    "        # CSV Format\n",
    "        csv_path = os.path.join(\n",
    "            self.config['feature_dir'],\n",
    "            f\"{video_name}_seg{segment_idx}_features.csv\"\n",
    "        )\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['frame', 'label', 'spatial', 'temporal', 'tracked', 'group'])\n",
    "            for idx, feat in enumerate(features):\n",
    "                writer.writerow([\n",
    "                    idx,\n",
    "                    label,\n",
    "                    json.dumps(feat['spatial']),\n",
    "                    json.dumps(feat['temporal']),\n",
    "                    json.dumps(feat['tracked']),\n",
    "                    json.dumps(feat['group'])\n",
    "                ])\n",
    "        \n",
    "        # NPZ Format\n",
    "        npz_path = os.path.join(\n",
    "            self.config['feature_dir'],\n",
    "            f\"{video_name}_seg{segment_idx}_features.npz\"\n",
    "        )\n",
    "        np.savez_compressed(\n",
    "            npz_path,\n",
    "            spatial=np.array([f['spatial'] for f in features], dtype=object),\n",
    "            temporal=np.array([f['temporal'] for f in features]),\n",
    "            tracked=np.array([f['tracked'] for f in features], dtype=object),\n",
    "            group=np.array([f['group'] for f in features], dtype=object),\n",
    "            label=label\n",
    "        )\n",
    "    \n",
    "    def _save_metadata(self, video_name, segment_idx, metadata):\n",
    "        meta_path = os.path.join(\n",
    "            self.config['metadata_dir'],\n",
    "            f\"{video_name}_seg{segment_idx}_meta.json\"\n",
    "        )\n",
    "        with open(meta_path, 'w') as f:\n",
    "            json.dump(metadata, f)\n",
    "\n",
    "# -------------------- Execution -------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'video_dir': r'D:\\CLASS NOTES\\EPICS\\Model Dataset\\Anomaly-Videos',\n",
    "        'frame_dir': r'D:\\CLASS NOTES\\EPICS\\Model Dataset\\Extracted_Frames',\n",
    "        'feature_dir': r'D:\\CLASS NOTES\\EPICS\\Model Dataset\\Extracted_feautures',\n",
    "        'metadata_dir': './processed/metadata',\n",
    "        'annotation_path': r'D:\\CLASS NOTES\\EPICS\\Model Dataset\\Temporal_Anomaly_Annotation_edited.txt'\n",
    "    }\n",
    "    \n",
    "    pipeline = EnhancedInteractionPipeline(config)\n",
    "    pipeline.process_videos(config['annotation_path'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
