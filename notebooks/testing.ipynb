{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "class VideoAnomalyPredictor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.load_models()\n",
    "        self.initialize_components()\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load trained model and preprocessing components\"\"\"\n",
    "        self.model = torch.load(self.config['model_save_path'], map_location=self.device)\n",
    "        self.model.eval()\n",
    "        self.scaler = joblib.load(self.config['scaler_save_path'])\n",
    "        self.label_mapping = joblib.load('label_mapping.joblib')\n",
    "        self.reverse_mapping = {v: k for k, v in self.label_mapping.items()}\n",
    "        \n",
    "        # Load detection models\n",
    "        self.yolo = YOLO('yolov8x-seg.pt')\n",
    "        self.mp_pose = mp.solutions.pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            model_complexity=2\n",
    "        )\n",
    "\n",
    "    def initialize_components(self):\n",
    "        \"\"\"Initialize feature tracking components\"\"\"\n",
    "        self.frame_buffer = deque(maxlen=32)\n",
    "        self.tracker = InteractionTracker(window_size=30, feature_dim=4)\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Process frame for feature extraction\"\"\"\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def extract_features(self, frame):\n",
    "        \"\"\"Extract features from a single frame\"\"\"\n",
    "        # Detect people\n",
    "        results = self.yolo(frame)[0]\n",
    "        detections = []\n",
    "        \n",
    "        if results.boxes is not None and len(results.boxes) > 0:\n",
    "            person_indices = [i for i, cls in enumerate(results.boxes.cls) if int(cls) == 0]\n",
    "            \n",
    "            for idx in person_indices:\n",
    "                box = results.boxes[idx]\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                cropped = frame[y1:y2, x1:x2]\n",
    "                \n",
    "                # Get pose keypoints\n",
    "                results_pose = self.mp_pose.process(cropped)\n",
    "                if results_pose.pose_landmarks:\n",
    "                    kps = np.array([[lm.x, lm.y, lm.z, lm.visibility] \n",
    "                                   for lm in results_pose.pose_landmarks.landmark])\n",
    "                else:\n",
    "                    kps = np.zeros((33, 4))\n",
    "                \n",
    "                detections.append({\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'keypoints': kps,\n",
    "                    'confidence': float(box.conf)\n",
    "                })\n",
    "        \n",
    "        # Calculate interactions and temporal features\n",
    "        interactions = self.calculate_interactions(detections)\n",
    "        temporal = self.get_temporal_features(frame)\n",
    "        tracked = self.tracker.update(interactions)\n",
    "        group = self.tracker.group_analysis(detections)\n",
    "        \n",
    "        return spatial, temporal, tracked, group\n",
    "\n",
    "    def predict_video(self, video_path, output_path=None):\n",
    "        \"\"\"Make predictions on a video file\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        predictions = []\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Preprocess and extract features\n",
    "            processed_frame = self.preprocess_frame(frame)\n",
    "            spatial, temporal, tracked, group = self.extract_features(processed_frame)\n",
    "            \n",
    "            # Combine and normalize features\n",
    "            features = np.concatenate([spatial, temporal, tracked, group])\n",
    "            features = self.scaler.transform([features])\n",
    "            \n",
    "            # Make prediction\n",
    "            with torch.no_grad():\n",
    "                tensor = torch.FloatTensor(features).to(self.device)\n",
    "                outputs = self.model(tensor)\n",
    "                pred = torch.argmax(outputs).item()\n",
    "                label = self.reverse_mapping[pred]\n",
    "                predictions.append(label)\n",
    "            \n",
    "            # Visualization\n",
    "            cv2.putText(frame, f\"Prediction: {label}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            if output_path:\n",
    "                cv2.imwrite(os.path.join(output_path, f\"frame_{len(predictions):04d}.jpg\"), frame)\n",
    "            else:\n",
    "                cv2.imshow('Prediction', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return self.aggregate_predictions(predictions)\n",
    "\n",
    "    def aggregate_predictions(self, predictions):\n",
    "        \"\"\"Combine frame predictions into final video prediction\"\"\"\n",
    "        counts = np.bincount(predictions)\n",
    "        final_label = self.reverse_mapping[np.argmax(counts)]\n",
    "        confidence = np.max(counts) / len(predictions)\n",
    "        return final_label, confidence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"model_save_path\": \"final_model.pth\",\n",
    "        \"scaler_save_path\": \"final_scaler.joblib\",\n",
    "        \"class_pattern\": r\"^([A-Za-z]+)\\d+\",\n",
    "        \"interaction_weights\": {\n",
    "            \"tracked\": 2.5,\n",
    "            \"group\": 3.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    predictor = VideoAnomalyPredictor(config)\n",
    "    video_path = \"path/to/your/video.mp4\"\n",
    "    prediction, confidence = predictor.predict_video(video_path)\n",
    "    print(f\"Video Prediction: {prediction} (Confidence: {confidence:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
