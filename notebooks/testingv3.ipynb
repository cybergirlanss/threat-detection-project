{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converted models to half-precision\n",
      "C:\\Users\\yacoo\\AppData\\Local\\Temp\\ipykernel_16776\\4190698083.py:91: RuntimeWarning: overflow encountered in cast\n",
      "  self.scaler.var_ = self.scaler.var_.astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 20.0ms\n",
      "Speed: 1.0ms preprocess, 20.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 20.0ms\n",
      "Speed: 0.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 19.4ms\n",
      "Speed: 0.0ms preprocess, 19.4ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 40.0ms\n",
      "Speed: 0.0ms preprocess, 40.0ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 19.0ms\n",
      "Speed: 0.0ms preprocess, 19.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 17.5ms\n",
      "Speed: 0.0ms preprocess, 17.5ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 19.0ms\n",
      "Speed: 0.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 24.0ms\n",
      "Speed: 0.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Insufficient frames for temporal features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 25.0ms\n",
      "Speed: 0.0ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 25.0ms\n",
      "Speed: 0.0ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 1 truck, 35.0ms\n",
      "Speed: 1.0ms preprocess, 35.0ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 35.0ms\n",
      "Speed: 0.0ms preprocess, 35.0ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 21.0ms\n",
      "Speed: 0.0ms preprocess, 21.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 21.0ms\n",
      "Speed: 0.0ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 19.0ms\n",
      "Speed: 0.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 17.5ms\n",
      "Speed: 0.0ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 19.0ms\n",
      "Speed: 0.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 17.5ms\n",
      "Speed: 0.0ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 39.0ms\n",
      "Speed: 0.0ms preprocess, 39.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 3 cars, 39.5ms\n",
      "Speed: 0.0ms preprocess, 39.5ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 4 cars, 1 bus, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 4 cars, 1 bus, 38.5ms\n",
      "Speed: 0.0ms preprocess, 38.5ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 4 cars, 1 bus, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 3 cars, 39.5ms\n",
      "Speed: 0.0ms preprocess, 39.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 3 persons, 3 cars, 16.5ms\n",
      "Speed: 0.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 1 person, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.5ms\n",
      "Speed: 0.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.3ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 39.0ms\n",
      "Speed: 0.0ms preprocess, 39.0ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 38.2ms\n",
      "Speed: 1.0ms preprocess, 38.2ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 4 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.5ms\n",
      "Speed: 0.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.5ms\n",
      "Speed: 1.0ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 2 persons, 3 cars, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Final Prediction: {'predicted_class': 'explosion', 'confidence': 0.8640488386154175, 'top_candidates': [('explosion', 0.86404884), ('shooting', 0.09886869), ('roadaccidents', 0.01885575)], 'class_distribution': {'abuse': {'count': 77, 'average_confidence': 0.0026872254}, 'arrest': {'count': 77, 'average_confidence': 0.008822179}, 'arson': {'count': 77, 'average_confidence': 0.001941057}, 'burglary': {'count': 77, 'average_confidence': 0.0040689395}, 'explosion': {'count': 77, 'average_confidence': 0.86404884}, 'roadaccidents': {'count': 77, 'average_confidence': 0.01885575}, 'robbery': {'count': 77, 'average_confidence': 0.00070725573}, 'shooting': {'count': 77, 'average_confidence': 0.09886869}}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import deque\n",
    "from itertools import combinations\n",
    "from torch import nn\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from pytorchvideo.models.hub import slow_r50\n",
    "import logging\n",
    "from scipy.stats import mode\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InteractionAwareModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "class VideoAnomalyPredictor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.half_precision = torch.cuda.is_available()\n",
    "        self.load_models()\n",
    "        self.initialize_components()\n",
    "        \n",
    "        self.feature_config = {\n",
    "            \"feature_dims\": {\n",
    "                \"spatial\": 36,\n",
    "                \"temporal\": 10,\n",
    "                \"tracked\": 4\n",
    "            },\n",
    "            \"interaction_weights\": {\n",
    "                \"tracked\": 2.5,\n",
    "                \"group\": 3.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Load models with dtype consistency checks\"\"\"\n",
    "        try:\n",
    "            # Load preprocessing components\n",
    "            self.scaler = joblib.load(self.config['scaler_save_path'])\n",
    "            self.label_mapping = joblib.load(self.config['label_mapping_path'])\n",
    "            self.reverse_mapping = {v: k for k, v in self.label_mapping.items()}\n",
    "\n",
    "            # Initialize main model\n",
    "            self.model = InteractionAwareModel(\n",
    "                input_size=self.scaler.n_features_in_,\n",
    "                num_classes=len(self.label_mapping)\n",
    "            ).to(self.device)\n",
    "            \n",
    "            state_dict = torch.load(self.config['model_save_path'], map_location=self.device)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            self.model.eval()\n",
    "\n",
    "            # Initialize detection models\n",
    "            self.yolo = YOLO('yolov8x-seg.pt').to(self.device)\n",
    "            self.mp_pose = mp.solutions.pose.Pose(\n",
    "                static_image_mode=False,\n",
    "                min_detection_confidence=0.5,\n",
    "                model_complexity=2)\n",
    "\n",
    "            # Initialize 3D CNN\n",
    "            self.c3d_model = slow_r50(pretrained=True).eval().to(self.device)\n",
    "\n",
    "            # Handle half-precision\n",
    "            if self.half_precision:\n",
    "                self.model.half()\n",
    "                self.c3d_model.half()\n",
    "                self.yolo.model.half()\n",
    "                logger.info(\"Converted models to half-precision\")\n",
    "                \n",
    "                # Convert scaler parameters to float16\n",
    "                self.scaler.mean_ = self.scaler.mean_.astype(np.float16)\n",
    "                self.scaler.var_ = self.scaler.var_.astype(np.float16)\n",
    "                self.scaler.scale_ = self.scaler.scale_.astype(np.float16)\n",
    "\n",
    "            logger.debug(f\"Model dtype: {next(self.model.parameters()).dtype}\")\n",
    "            logger.debug(f\"YOLO dtype: {next(self.yolo.model.parameters()).dtype}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model loading failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_components(self):\n",
    "        \"\"\"Initialize buffers and trackers\"\"\"\n",
    "        self.frame_buffer = deque(maxlen=32)\n",
    "        self.confidence_history = deque(maxlen=16)\n",
    "        self.tracking_history = deque(maxlen=5)\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Process frame with GPU acceleration support\"\"\"\n",
    "        try:\n",
    "            # Use CUDA-accelerated resize if available\n",
    "            if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
    "                gpu_frame = cv2.cuda_GpuMat()\n",
    "                gpu_frame.upload(frame)\n",
    "                resized = cv2.cuda.resize(gpu_frame, (224, 224)).download()\n",
    "            else:\n",
    "                resized = cv2.resize(frame, (224, 224))\n",
    "                \n",
    "            frame = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "            tensor = torch.from_numpy(frame).permute(2, 0, 1)\n",
    "            tensor = tensor.to(self.device).float() / 255.0\n",
    "            \n",
    "            if self.half_precision:\n",
    "                tensor = tensor.half()\n",
    "                \n",
    "            return tensor\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Frame preprocessing failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def vector_angle(self, v1, v2):\n",
    "        \"\"\"Calculate angle between vectors with stability\"\"\"\n",
    "        cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-9)\n",
    "        return np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "\n",
    "    def extract_spatial_features(self, people):\n",
    "        \"\"\"Spatial features with kinematic validation\"\"\"\n",
    "        features = []\n",
    "        try:\n",
    "            if len(people) >= 2:\n",
    "                logger.debug(f\"Processing spatial features for {len(people)} people\")\n",
    "                p1, p2 = people[:2]\n",
    "                \n",
    "                # Joint position features\n",
    "                joint_pairs = [(15, 0), (27, 27), (11, 23), (12, 24)]\n",
    "                for j1, j2 in joint_pairs:\n",
    "                    kp1 = p1['keypoints'][j1][:3]\n",
    "                    kp2 = p2['keypoints'][j2][:3]\n",
    "                    features.append(np.linalg.norm(kp1 - kp2))\n",
    "                \n",
    "                # Velocity features\n",
    "                if len(self.tracking_history) > 1:\n",
    "                    prev_pos = self.tracking_history[-1]\n",
    "                    curr_pos = [p['bbox_center'] for p in [p1, p2]]\n",
    "                    dt = 1/30  # Assuming 30 FPS\n",
    "                    velocities = [\n",
    "                        np.linalg.norm(np.array(curr) - np.array(prev)) / dt\n",
    "                        for curr, prev in zip(curr_pos, prev_pos)\n",
    "                    ]\n",
    "                    features.extend(velocities)\n",
    "                \n",
    "                self.tracking_history.append([p['bbox_center'] for p in [p1, p2]])\n",
    "            \n",
    "            # Pad to 36 features if needed\n",
    "            features += [0.0] * (36 - len(features))\n",
    "            return features[:36]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Spatial feature error: {str(e)}\")\n",
    "            return [0.0] * 36\n",
    "\n",
    "    def extract_temporal_features(self):\n",
    "        \"\"\"Temporal features with buffer validation\"\"\"\n",
    "        try:\n",
    "            if len(self.frame_buffer) < 16:\n",
    "                logger.warning(\"Insufficient frames for temporal features\")\n",
    "                return np.zeros(10)\n",
    "            \n",
    "            # Process sequence through 3D CNN\n",
    "            sequence = torch.stack(list(self.frame_buffer))  # [T, C, H, W]\n",
    "            sequence = sequence.permute(1, 0, 2, 3)        # [C, T, H, W]\n",
    "            sequence = sequence.unsqueeze(0)                # [1, C, T, H, W]\n",
    "            \n",
    "            if self.half_precision:\n",
    "                sequence = sequence.half()\n",
    "                \n",
    "            features = self.c3d_model(sequence).squeeze().cpu().numpy()\n",
    "            return features[:10]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Temporal feature error: {str(e)}\")\n",
    "            return np.zeros(10)\n",
    "\n",
    "    def process_interaction_features(self, spatial_features):\n",
    "        \"\"\"Interaction features with dynamic weighting\"\"\"\n",
    "        try:\n",
    "            # Dynamic weight adjustment based on feature confidence\n",
    "            base_weights = np.array([2.5, 1.0, 1.0, 3.0])\n",
    "            confidence = np.clip(np.mean(spatial_features[:3]), 0, 1)\n",
    "            weights = base_weights * (1 + confidence)\n",
    "            \n",
    "            return [\n",
    "                spatial_features[0] * weights[0],\n",
    "                spatial_features[1] * weights[1],\n",
    "                spatial_features[2] * weights[2],\n",
    "                weights[3]  # Group weight\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Interaction feature error: {str(e)}\")\n",
    "            return [0.0] * 4\n",
    "\n",
    "    def extract_features(self, frame_tensor):\n",
    "        \"\"\"Complete feature pipeline with validation\"\"\"\n",
    "        try:\n",
    "            people = []\n",
    "            with torch.no_grad():\n",
    "                results = self.yolo(frame_tensor[None])[0]\n",
    "                if results.boxes is not None:\n",
    "                    for idx in (i for i, cls in enumerate(results.boxes.cls) if int(cls) == 0):\n",
    "                        box = results.boxes[idx]\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                        cropped = frame_tensor[:, y1:y2, x1:x2].cpu().numpy().transpose(1, 2, 0)\n",
    "                        \n",
    "                        results_pose = self.mp_pose.process((cropped * 255).astype(np.uint8))\n",
    "                        kps = np.zeros((33, 4)) if not results_pose.pose_landmarks else np.array([\n",
    "                            [lm.x, lm.y, lm.z, lm.visibility] \n",
    "                            for lm in results_pose.pose_landmarks.landmark\n",
    "                        ])\n",
    "                        people.append({\n",
    "                            'keypoints': kps,\n",
    "                            'bbox_center': [(x1+x2)//2, (y1+y2)//2],\n",
    "                            'confidence': float(box.conf)\n",
    "                        })\n",
    "\n",
    "            # Feature extraction and validation\n",
    "            spatial = self.extract_spatial_features(people)\n",
    "            temporal = self.extract_temporal_features()\n",
    "            interaction = self.process_interaction_features(spatial)\n",
    "            \n",
    "            combined = np.concatenate([\n",
    "                spatial[:36],\n",
    "                temporal[:10],\n",
    "                interaction[:4]\n",
    "            ])\n",
    "            \n",
    "            # Validate features\n",
    "            if len(combined) != self.scaler.n_features_in_:\n",
    "                logger.error(f\"Feature dimension mismatch: {len(combined)} vs {self.scaler.n_features_in_}\")\n",
    "                return np.zeros(self.scaler.n_features_in_)\n",
    "            \n",
    "            if np.all(np.abs(combined) < 1e-6):\n",
    "                logger.error(\"All features are zero!\")\n",
    "                return np.zeros(self.scaler.n_features_in_)\n",
    "                \n",
    "            if np.any(np.isnan(combined)):\n",
    "                logger.error(\"NaN values detected in features!\")\n",
    "                return np.zeros(self.scaler.n_features_in_)\n",
    "            \n",
    "            return combined\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Feature extraction failed: {str(e)}\")\n",
    "            return np.zeros(self.scaler.n_features_in_)\n",
    "\n",
    "    def predict_video(self, video_path):\n",
    "        \"\"\"Enhanced prediction pipeline with resource management\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            logger.error(f\"Failed to open video: {video_path}\")\n",
    "            return {'class': 'unknown', 'confidence': 0.0}\n",
    "        \n",
    "        predictions = []\n",
    "        class_confidence = {cls: [] for cls in self.label_mapping.keys()}\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame_tensor = self.preprocess_frame(frame)\n",
    "                if frame_tensor is None:\n",
    "                    continue\n",
    "                \n",
    "                self.frame_buffer.append(frame_tensor)\n",
    "                \n",
    "                try:\n",
    "                    with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=self.half_precision):\n",
    "                        features = self.extract_features(frame_tensor)\n",
    "                        \n",
    "                        # Skip frame if features are invalid\n",
    "                        if np.all(features == 0):\n",
    "                            logger.warning(\"Skipping frame with invalid features\")\n",
    "                            continue\n",
    "                            \n",
    "                        # Normalization and prediction\n",
    "                        features = self.scaler.transform([features])\n",
    "                        tensor = torch.tensor(features, \n",
    "                                            dtype=torch.float16 if self.half_precision else torch.float32)\n",
    "                        tensor = tensor.to(self.device)\n",
    "                        \n",
    "                        outputs = self.model(tensor)\n",
    "                        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "                        \n",
    "                        # Track confidence for all classes\n",
    "                        for i, prob in enumerate(probs):\n",
    "                            class_name = self.reverse_mapping[i]\n",
    "                            class_confidence[class_name].append(prob)\n",
    "                            \n",
    "                        # Adaptive confidence threshold\n",
    "                        current_max = np.max(probs)\n",
    "                        threshold = 0.4 if current_max > 0.7 else 0.25\n",
    "                        \n",
    "                        if current_max > threshold:\n",
    "                            pred = np.argmax(probs)\n",
    "                            predictions.append(pred)\n",
    "\n",
    "                        # Visualization\n",
    "                        display_frame = cv2.resize(frame, (1280, 720))\n",
    "                        avg_confidence = np.mean(self.confidence_history) if self.confidence_history else 0.0\n",
    "                        label = self.reverse_mapping.get(np.argmax(probs), \"unknown\")\n",
    "                        cv2.putText(display_frame, f\"{label} ({avg_confidence:.2f})\", \n",
    "                                  (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "                        cv2.imshow('Prediction', display_frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Frame processing error: {str(e)}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            # Cleanup models\n",
    "            self.yolo = None\n",
    "            self.mp_pose.close()\n",
    "            return self.aggregate_predictions(class_confidence)\n",
    "\n",
    "    def aggregate_predictions(self, class_confidence):\n",
    "        \"\"\"Robust aggregation with confidence analysis\"\"\"\n",
    "        try:\n",
    "            # Calculate mean confidence per class\n",
    "            mean_conf = {}\n",
    "            for cls, confs in class_confidence.items():\n",
    "                mean_conf[cls] = np.mean(confs) if confs else 0.0\n",
    "                \n",
    "            # Sort classes by confidence\n",
    "            sorted_classes = sorted(mean_conf.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Create distribution dictionary\n",
    "            distribution = {}\n",
    "            for cls, confs in class_confidence.items():\n",
    "                count = len(confs)\n",
    "                avg = np.mean(confs) if count > 0 else 0.0\n",
    "                distribution[cls] = {'count': count, 'average_confidence': avg}\n",
    "            \n",
    "            return {\n",
    "                'predicted_class': sorted_classes[0][0],\n",
    "                'confidence': float(sorted_classes[0][1]),\n",
    "                'top_candidates': sorted_classes[:3],\n",
    "                'class_distribution': distribution\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Aggregation failed: {str(e)}\")\n",
    "            return {\n",
    "                'predicted_class': 'unknown',\n",
    "                'confidence': 0.0,\n",
    "                'class_distribution': {}\n",
    "            }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"model_save_path\": r\"D:\\CLASS NOTES\\EPICS\\Model\\final_model.pth\",\n",
    "        \"scaler_save_path\": r\"D:\\CLASS NOTES\\EPICS\\Model\\final_scaler.joblib\",\n",
    "        \"label_mapping_path\": r\"D:\\CLASS NOTES\\EPICS\\Model\\label_mapping.joblib\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        predictor = VideoAnomalyPredictor(config)\n",
    "        result = predictor.predict_video(r\"D:\\CLASS NOTES\\EPICS\\Model Dataset\\Anomaly-Videos\\Shooting002_x264.mp4\")\n",
    "        print(f\"Final Prediction: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error: {str(e)}\")\n",
    "        cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
